#!/usr/bin/env python3
"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Honda CB500F/CB500X –Ω–∞ Facebook Marketplace
–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
"""

import asyncio
import json
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass, asdict
import aiohttp

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ
    pass

from fb_scraper import FacebookMarketplaceScraper
from telegram_notifier import TelegramNotifier
from data_storage import DataStorage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class SearchRegion:
    """–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞"""
    name: str
    market_id: str
    radius_miles: int = 150

@dataclass
class ListingChange:
    """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
    change_type: str  # 'new', 'removed', 'price_change'
    listing_id: str
    old_data: Optional[Dict] = None
    new_data: Optional[Dict] = None
    price_diff: Optional[float] = None

class CB500Monitor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    
    # –†–µ–≥–∏–æ–Ω—ã –ø–æ–∏—Å–∫–∞ –≤ —Ä–∞–¥–∏—É—Å–µ 150-200 –º–∏–ª—å –æ—Ç Summit, NJ
    SEARCH_REGIONS = [
        SearchRegion("New York Metro", "103727996333163"),  # NYC area
        SearchRegion("North Jersey", "109824442381734"),     # North NJ
        SearchRegion("Central Jersey", "104052089631773"),   # Central NJ
        SearchRegion("Philadelphia", "112724858717904"),     # Philly area
        SearchRegion("South Jersey", "104095516327164"),     # South NJ
        SearchRegion("Pennsylvania", "110396605639860"),     # PA general
        SearchRegion("Maryland", "106441769381283"),         # MD general
        SearchRegion("Virginia", "113301148664532"),         # VA general
        SearchRegion("Connecticut", "109415472407501"),      # CT general
        SearchRegion("Delaware", "106015479455140"),         # DE general
    ]
    
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    SEARCH_QUERIES = ["cb500f", "cb500x", "cb 500f", "cb 500x"]
    
    # –¶–µ–Ω–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
    MIN_PRICE = 3500
    MAX_PRICE = 5800
    
    def __init__(self, data_dir: Path, cookies_path: Path):
        self.data_dir = data_dir
        self.cookies_path = cookies_path
        self.storage = DataStorage(data_dir)
        self.scraper = FacebookMarketplaceScraper(cookies_path)
        self.telegram = TelegramNotifier()
        self.verbose_logging = os.getenv('VERBOSE_LOGGING', 'false').lower() == 'true'
        self.days_since_listed = int(os.getenv('DAYS_SINCE_LISTED', '14'))
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def extract_price(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        
        # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ $XXXX
        price_matches = re.findall(r'\$(\d{1,2}(?:,\d{3})*|\d{3,6})', text)
        
        if not price_matches:
            return None
        
        try:
            # –ë–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ü–µ–Ω–∞)
            prices = [float(match.replace(',', '')) for match in price_matches]
            prices = [p for p in prices if self.MIN_PRICE <= p <= self.MAX_PRICE]
            return max(prices) if prices else None
        except ValueError:
            return None
    
    def is_relevant_listing(self, listing: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        text = ' '.join([
            listing.get('title', ''),
            listing.get('price_text', ''),
            listing.get('description', '')
        ]).lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CB500
        has_cb500 = any(query in text for query in ['cb500f', 'cb500x', 'cb 500f', 'cb 500x'])
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
        excludes = ['cb650', 'cb300', 'cb1000', 'cbr500', 'cbr600']
        has_exclude = any(exclude in text for exclude in excludes)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—É
        price = self.extract_price(listing.get('price_text', '') + listing.get('title', ''))
        valid_price = price is not None and self.MIN_PRICE <= price <= self.MAX_PRICE
        
        return has_cb500 and not has_exclude and valid_price
    
    async def scrape_region(self, region: SearchRegion, query: str) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–º —Ä–µ–≥–∏–æ–Ω–µ"""
        url = f"https://www.facebook.com/marketplace/{region.market_id}/search"
        params = {
            "daysSinceListed": str(self.days_since_listed),  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
            "query": query,
            "sortBy": "creation_time_descend",
            "exact": "false"
        }
        
        try:
            logger.info(f"Scraping {region.name} for '{query}'")
            listings = await self.scraper.scrape_search(url, params, verbose=self.verbose_logging)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            relevant = [listing for listing in listings if self.is_relevant_listing(listing)]
            
            if self.verbose_logging:
                logger.info(f"Found {len(listings)} total listings, {len(relevant)} relevant in {region.name}")
            else:
                logger.info(f"Found {len(relevant)} relevant listings in {region.name}")
            return relevant
            
        except Exception as e:
            logger.error(f"Error scraping {region.name}: {e}")
            await self.telegram.send_error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ {region.name}: {e}")
            return []
    
    async def scrape_all_regions(self) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏—Ç –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã –∏ –∑–∞–ø—Ä–æ—Å—ã"""
        all_listings = []
        seen_ids = set()
        region_stats = {}
        
        for region in self.SEARCH_REGIONS:
            region_count = 0
            for query in self.SEARCH_QUERIES:
                try:
                    listings = await self.scrape_region(region, query)
                    region_count += len(listings)
                    
                    # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    for listing in listings:
                        listing_id = listing.get('listing_id')
                        if listing_id and listing_id not in seen_ids:
                            listing['search_region'] = region.name
                            listing['search_query'] = query
                            all_listings.append(listing)
                            seen_ids.add(listing_id)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"Error in region {region.name}, query '{query}': {e}")
                    continue
            
            region_stats[region.name] = region_count
            if region_count > 0:
                logger.info(f"Region {region.name}: {region_count} total listings found")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info(f"Total unique listings found: {len(all_listings)}")
        logger.info(f"Regional breakdown: {region_stats}")
        logger.info(f"Duplicates removed: {sum(region_stats.values()) - len(all_listings)}")
        
        return all_listings
    
    def detect_changes(self, old_listings: Dict[str, Dict], new_listings: List[Dict]) -> List[ListingChange]:
        """–î–µ—Ç–µ–∫—Ç–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —Å—Ç–∞—Ä—ã–º–∏ –∏ –Ω–æ–≤—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏"""
        changes = []
        new_listings_dict = {listing['listing_id']: listing for listing in new_listings}
        
        # –ù–æ–≤—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        for listing_id, listing in new_listings_dict.items():
            if listing_id not in old_listings:
                changes.append(ListingChange(
                    change_type='new',
                    listing_id=listing_id,
                    new_data=listing
                ))
        
        # –£–¥–∞–ª–µ–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        for listing_id, listing in old_listings.items():
            if listing_id not in new_listings_dict:
                changes.append(ListingChange(
                    change_type='removed',
                    listing_id=listing_id,
                    old_data=listing
                ))
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω
        for listing_id in set(old_listings.keys()) & set(new_listings_dict.keys()):
            old_listing = old_listings[listing_id]
            new_listing = new_listings_dict[listing_id]
            
            old_price = self.extract_price(old_listing.get('price_text', '') + old_listing.get('title', ''))
            new_price = self.extract_price(new_listing.get('price_text', '') + new_listing.get('title', ''))
            
            if old_price and new_price and abs(old_price - new_price) > 50:  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ > $50
                changes.append(ListingChange(
                    change_type='price_change',
                    listing_id=listing_id,
                    old_data=old_listing,
                    new_data=new_listing,
                    price_diff=new_price - old_price
                ))
        
        return changes
    
    async def send_change_notifications(self, changes: List[ListingChange]):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö"""
        if not changes:
            return
        
        # –°–æ–∑–¥–∞–µ–º —Å–∞–º–º–∞—Ä–∏ –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
        new_count = sum(1 for c in changes if c.change_type == 'new')
        removed_count = sum(1 for c in changes if c.change_type == 'removed')
        price_change_count = sum(1 for c in changes if c.change_type == 'price_change')
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Å–∞–º–º–∞—Ä–∏
        await self.telegram.send_changes_summary(changes, new_count, removed_count, price_change_count)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        await asyncio.sleep(1)
    
    async def send_no_changes_summary(self, listings: List[Dict[str, Any]]):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –∫–æ–≥–¥–∞ –Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –∫–∞–∫ –≤ view_database.py
            summary_text = self.create_detailed_summary(listings)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É –≤ Telegram
            await self.telegram.send_message(f"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ (–∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ—Ç)\n\n{summary_text}")
            
        except Exception as e:
            logger.error(f"Error sending no changes summary: {e}")
    
    def create_detailed_summary(self, listings: List[Dict[str, Any]]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º Telegram"""
        if not listings:
            return "üì≠ *–û–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ*"
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        regions = {}
        for listing in listings:
            region = listing.get('search_region', 'Unknown')
            if region not in regions:
                regions[region] = 0
            regions[region] += 1
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å–≤–æ–¥–∫–∏ —Å Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        summary = f"üìä *–°–í–û–î–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–• CB500F*\n"
        summary += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        summary += f"üèç *–í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π:* `{len(listings)}`\n\n"
        
        if regions:
            summary += "üìç *–ü–æ —Ä–µ–≥–∏–æ–Ω–∞–º:*\n"
            for region, count in sorted(regions.items()):
                summary += f"  ‚Ä¢ {region}: `{count}`\n"
            summary += "\n"
        
        summary += "üìã *–î–ï–¢–ê–õ–¨–ù–´–ô –°–ü–ò–°–û–ö –û–ë–™–Ø–í–õ–ï–ù–ò–ô*\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –∫–∞–∂–¥–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞)
        for i, listing in enumerate(listings, 1):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏ –ª–∏–º–∏—Ç Telegram (4096 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(summary) > 3200:  # –û—Å—Ç–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∑–∞–ø–∞—Å–∞
                remaining = len(listings) - i + 1
                summary += f"... –∏ –µ—â–µ *{remaining}* –æ–±—ä—è–≤–ª–µ–Ω–∏–π\n"
                summary += f"ÔøΩ _–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫: kubectl exec ... -- python /app/view_database.py --detailed_"
                break
                
            summary += f"üèç *{i}. "
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ID
            listing_id = listing.get('id') or listing.get('listing_id') or 'N/A'
            summary += f"ID:* `{listing_id}`\n"
            
            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —É–ª—É—á—à–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            title = listing.get('title', 'N/A')
            if title and title != 'N/A':
                if len(title) > 60:
                    title = title[:57] + "..."
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –≥–æ–¥ –∏ –º–æ–¥–µ–ª—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                year_match = re.search(r'(20\d{2})', title)
                model_match = re.search(r'(CB\s*500[FX]?)', title, re.IGNORECASE)
                
                if year_match and model_match:
                    summary += f"üìÖ *{year_match.group(1)} Honda {model_match.group(1).upper()}*\n"
                else:
                    summary += f"üè∑ _{title}_\n"
            
            # –¶–µ–Ω–∞ —Å –ª—É—á—à–∏–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            price = listing.get('price_text', 'N/A')
            if price and price != 'N/A':
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–Ω—É
                price_match = re.search(r'\$[\d,]+', price)
                if price_match:
                    summary += f"üí∞ *{price_match.group()}*\n"
                else:
                    summary += f"üí∞ {price}\n"
            
            # –õ–æ–∫–∞—Ü–∏—è
            location = listing.get('location', 'N/A')
            if location and location != 'N/A':
                summary += f"üìç {location}\n"
            
            # –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            region = listing.get('search_region', 'N/A')
            if region and region != 'N/A':
                summary += f"üó∫ _{region}_\n"
            
            # –í—Ä–µ–º—è –≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            scraped_at = listing.get('scraped_at')
            if scraped_at:
                try:
                    if isinstance(scraped_at, (int, float)):
                        # Unix timestamp
                        dt = datetime.fromtimestamp(scraped_at)
                    else:
                        # –°—Ç—Ä–æ–∫–∞ –¥–∞—Ç—ã
                        dt = datetime.fromisoformat(str(scraped_at).replace('Z', '+00:00'))
                    
                    formatted_time = dt.strftime('%d.%m.%Y %H:%M')
                    summary += f"‚è∞ {formatted_time}\n"
                except:
                    summary += f"‚è∞ _{scraped_at}_\n"
            
            # –†–∞–±–æ—á–∞—è —Å—Å—ã–ª–∫–∞
            url = listing.get('url')
            if url:
                # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã
                clean_url = url.split('?')[0]
                if '/marketplace/item/' in clean_url:
                    summary += f"üîó [–û—Ç–∫—Ä—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ]({clean_url})\n"
                else:
                    summary += f"üîó {clean_url}\n"
            
            summary += "\n"
        
        return summary
    
    async def run_monitor_cycle(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        timestamp = datetime.now()
        logger.info(f"Starting monitor cycle at {timestamp}")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            old_listings = self.storage.load_current_state()
            
            # –°–∫—Ä–∞–ø–∏–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            new_listings = await self.scrape_all_regions()
            
            if not new_listings:
                logger.warning("No listings found - possible scraping issue")
                await self.telegram.send_error("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π - –≤–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–æ–º")
                return
            
            # –î–µ—Ç–µ–∫—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            changes = self.detect_changes(old_listings, new_listings)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self.storage.save_current_state(new_listings)
            self.storage.save_historical_data(new_listings, timestamp)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
            if changes:
                logger.info(f"Found {len(changes)} changes")
                await self.send_change_notifications(changes)
            else:
                logger.info("No changes detected")
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –∫–æ–≥–¥–∞ –Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
                await self.send_no_changes_summary(new_listings)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É (—Ä–∞–∑ –≤ –¥–µ–Ω—å –≤ 9 —É—Ç—Ä–∞)
            if timestamp.hour == 9 and timestamp.minute < 5:
                await self.telegram.send_daily_summary(new_listings)
            
        except Exception as e:
            logger.error(f"Error in monitor cycle: {e}")
            await self.telegram.send_error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
            raise

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    data_dir = Path(os.getenv('DATA_DIR', '/app/data'))
    cookies_path = Path(os.getenv('COOKIES_PATH', '/app/data/cookies.json'))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
    if not cookies_path.exists():
        logger.error(f"Cookies file not found: {cookies_path}")
        return
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä
    monitor = CB500Monitor(data_dir, cookies_path)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    await monitor.run_monitor_cycle()

if __name__ == "__main__":
    asyncio.run(main())
